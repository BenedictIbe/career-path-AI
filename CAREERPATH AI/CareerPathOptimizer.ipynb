{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Title**: Career Optimizer Model\r\n",
        "\r\n",
        "**Description**: Here we aim to run job title normalization (Text classification and clustering), CV skill extraction, and filtration based recommendation model on the dataset to recommend a candidate to the skills they require to migrate to their desired career\r\n",
        "\r\n",
        "**Author**: Benedict Ibe\r\n",
        "\r\n",
        "**Date Created**: 15/08/2023"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "outputs": [],
      "execution_count": 77,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\n",
        "\n",
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
        "candidate_skilllevel= spark.sql(\"SELECT \\\n",
        "Id                                          as Cand_ID, \\\n",
        "recruit_candidatecontact                    as Candidate_contactno, \\\n",
        "crimson_proficiency                         as Cand_proficiency, \\\n",
        "crimson_requirement                         as Cand_requirement, \\\n",
        "crimson_experience                          as Cand_experience, \\\n",
        "crimson_experienceperiod                    as Cand_exp_period, \\\n",
        "crimson_skill                               as Cand_skill, \\\n",
        "crimson_name                                as Cand_name, \\\n",
        "crimson_skilllevelid                        as Cand_skilllevel_ID, \\\n",
        "lower(crimson_skillname)                    as Cand_skill_name, \\\n",
        "crimson_level                               as Cand_level \\\n",
        "FROM dataverse_edensmithcon_org87f26120.crimson_skilllevel\")"
      ],
      "outputs": [],
      "execution_count": 78,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "ms_comments": [
          {
            "threadId": "03783d22-e6cc-4dff-a7a6-e294d0d0821f",
            "text": "Load the skills dataset",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690492217907,
            "modifiedDateUTC": 1690492217907,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "03783d22-e6cc-4dff-a7a6-e294d0d0821f": {
            "text": "%%pyspark",
            "start": {
              "line": 1,
              "column": 1
            },
            "end": {
              "line": 1,
              "column": 10
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_skilllevel.show()"
      ],
      "outputs": [],
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\n",
        "\n",
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
        "candidate_workhist =spark.sql(\"SELECT \\\n",
        "Id                                  as ID , \\\n",
        "recruit_candidatecontact            as Candidate_contact,\\\n",
        "crimson_startdate                   as Candidate_workstrt_date,\\\n",
        "crimson_enddate                     as Candidate_workend_date, \\\n",
        "crimson_workhistoryid               as WorkHistory_ID,\\\n",
        "lower(crimson_description)          as Candidate_work_description,\\\n",
        "crimson_jobtitle                    as Candidate_job_title ,\\\n",
        "crimson_name                        as Cand_workplace \\\n",
        "FROM dataverse_edensmithcon_org87f26120.crimson_workhistory\")"
      ],
      "outputs": [],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date"
      ],
      "outputs": [],
      "execution_count": 81,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_workhist.show()"
      ],
      "outputs": [],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we convert the date-time format to date\n",
        "df_hist = candidate_workhist.withColumn(\"Cnd_start_date\", to_date(\"Candidate_workstrt_date\"))\n",
        "df_hist.show()"
      ],
      "outputs": [],
      "execution_count": 83,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "5fd47b2b-c642-4011-bdf6-bb049e6cfc8e",
            "text": "Here we convert the date-time format to date",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690492358715,
            "modifiedDateUTC": 1690492358715,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "5fd47b2b-c642-4011-bdf6-bb049e6cfc8e": {
            "text": "df_hist = candidate_workhist.withColumn(\"Cnd_star",
            "start": {
              "line": 2,
              "column": 1
            },
            "end": {
              "line": 2,
              "column": 50
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we convert the date-time format to date\n",
        "df_hist2 = df_hist.withColumn(\"Cnd_end_date\", to_date(\"Candidate_workend_date\"))\n",
        "df_hist2.show()"
      ],
      "outputs": [],
      "execution_count": 84,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "a53e57de-a1fe-429c-a5a1-17728e643560",
            "text": "Here we convert the date-time format to date",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690492449872,
            "modifiedDateUTC": 1690492449872,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "a53e57de-a1fe-429c-a5a1-17728e643560": {
            "text": "withColumn",
            "start": {
              "line": 2,
              "column": 20
            },
            "end": {
              "line": 2,
              "column": 30
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we select the required columns\n",
        "df_hist3 = df_hist2.select('ID', 'WorkHistory_ID', 'Candidate_contact', 'Candidate_job_title', 'Cnd_start_date', 'Cnd_end_date')\n",
        "df_hist3.show()"
      ],
      "outputs": [],
      "execution_count": 85,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "c21259d2-4239-4270-a04f-808e56e6bd2d",
            "text": "Here we select the required columns",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690492481381,
            "modifiedDateUTC": 1690492481381,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "c21259d2-4239-4270-a04f-808e56e6bd2d": {
            "text": "WorkHistory_ID",
            "start": {
              "line": 2,
              "column": 35
            },
            "end": {
              "line": 2,
              "column": 49
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import datediff, floor\n",
        "from pyspark.sql.functions import lit"
      ],
      "outputs": [],
      "execution_count": 86,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GETTING THE NUMBER OF MONTHS BETWEEN THE START TIME AND THE END TIME"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hist4 = df_hist3.withColumn(\"Duration\", floor((datediff(df_hist3[\"Cnd_end_date\"], df_hist3[\"Cnd_start_date\"]) / 7)/4))\n",
        "df_hist4.show()"
      ],
      "outputs": [],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "a3a221a2-f343-4933-a126-a7c6edd223f0",
            "text": "Get the number of days between the start date and end date, then divide it to get the monthly number",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690492740426,
            "modifiedDateUTC": 1690492740426,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "a3a221a2-f343-4933-a126-a7c6edd223f0": {
            "text": "df_hist3",
            "start": {
              "line": 1,
              "column": 60
            },
            "end": {
              "line": 1,
              "column": 68
            }
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE THE PARAMETER TO SEARCH"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_titleSearch = 'data'"
      ],
      "outputs": [],
      "execution_count": 88,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SEARCH FOR THE WILD CHARACTER DEFINED IN THE PARAMETER"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hist5 = df_hist4.filter(df_hist4.Candidate_job_title.rlike(job_titleSearch)).sort(df_hist4.Candidate_job_title.asc())\n",
        "df_hist5.show(50)"
      ],
      "outputs": [],
      "execution_count": 89,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "432d6ca1-1424-4d0c-8366-6060ff15e276",
            "text": "Search for the defined wildcard in theÂ  Candidate_job_title column. Here we convert the date-time format to date using rlike",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690492863175,
            "modifiedDateUTC": 1690492945701,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "432d6ca1-1424-4d0c-8366-6060ff15e276": {
            "text": "50",
            "start": {
              "line": 2,
              "column": 15
            },
            "end": {
              "line": 2,
              "column": 17
            }
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_replace, lower\n",
        "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline"
      ],
      "outputs": [],
      "execution_count": 90,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "REMOVE SPECIAL CHARACTERS AS A PREPROCESSING STEP"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removal of special characters and conerting the the strings to lower case\n",
        "df_clean = df_hist4.withColumn(\"Candidate_job_title\", lower(regexp_replace(\"Candidate_job_title\", \"[^a-zA-Z0-9\\\\s]\", \"\")))\n",
        "df_clean.show()"
      ],
      "outputs": [],
      "execution_count": 91,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "63e2cb31-12c4-4df0-bd41-e29f9e473c72",
            "text": "Clean up the column by removing special characters and punctuations, as well as converting the strings to lower case",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690493055834,
            "modifiedDateUTC": 1690493055834,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "63e2cb31-12c4-4df0-bd41-e29f9e473c72": {
            "text": "show",
            "start": {
              "line": 3,
              "column": 10
            },
            "end": {
              "line": 3,
              "column": 14
            }
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TREAT NULL VALUES"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean2 = df_clean.na.drop(subset=[\"Candidate_job_title\"])"
      ],
      "outputs": [],
      "execution_count": 92,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "7b067015-5c9e-4054-9437-605854d07240",
            "text": "Remove missing values",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690493130964,
            "modifiedDateUTC": 1690493130964,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "7b067015-5c9e-4054-9437-605854d07240": {
            "text": "na",
            "start": {
              "line": 1,
              "column": 22
            },
            "end": {
              "line": 1,
              "column": 24
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter to explore analyst programmer job title\n",
        "df_clean3 = df_clean.filter(df_clean.Candidate_job_title == \"analystprogrammer  programmer\")\n",
        "df_clean3.show(100, truncate = False)"
      ],
      "outputs": [],
      "execution_count": 93,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD THE SAMPLE DATA TO BE USED FOR TRAINING"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\n",
        "df_salaries = spark.read.load('abfss://landing@esgadls2.dfs.core.windows.net/salaries_clean.csv', format='csv'\n",
        ",header=True\n",
        ")\n",
        "df_salaries.show()"
      ],
      "outputs": [],
      "execution_count": 94,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "ms_comments": [
          {
            "threadId": "0980b606-f9b7-4c8d-b56b-1f29d8e703ff",
            "text": "Load a sample dataset which we downloaded from kaggle to be used in training the data.",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690493194785,
            "modifiedDateUTC": 1690493194785,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "0980b606-f9b7-4c8d-b56b-1f29d8e703ff": {
            "text": ")",
            "start": {
              "line": 4,
              "column": 1
            },
            "end": {
              "line": 4,
              "column": 2
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select the required column from the sample dataset\n",
        "df_train = df_salaries.select('job_title', 'job_title_category')\n",
        "df_train.show()"
      ],
      "outputs": [],
      "execution_count": 95,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "765b7e67-f6c2-4658-9288-f64f37ca3dd4",
            "text": "Select the required column from the sample dataset",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690493233745,
            "modifiedDateUTC": 1690493233745,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "765b7e67-f6c2-4658-9288-f64f37ca3dd4": {
            "text": "job_title",
            "start": {
              "line": 2,
              "column": 32
            },
            "end": {
              "line": 2,
              "column": 41
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when"
      ],
      "outputs": [],
      "execution_count": 96,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Treat missing values in the dataset\r\n",
        "dfTrain_clean2 = df_train.na.drop(subset=[\"job_title\"])"
      ],
      "outputs": [],
      "execution_count": 97,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "d3806602-21bb-445e-acfa-a063bb200986",
            "text": "Treat missing values in the dataset",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690493262619,
            "modifiedDateUTC": 1690493262619,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "d3806602-21bb-445e-acfa-a063bb200986": {
            "text": "[\"job_title",
            "start": {
              "line": 2,
              "column": 42
            },
            "end": {
              "line": 2,
              "column": 53
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare the dataset by converting cell to its appropriate general form\n",
        "df_train2 = dfTrain_clean2.withColumn(\"job_title_category\",\n",
        "                   when(col(\"job_title\").rlike(\"analyst\"), \"Data Analyst\")\n",
        "                   .when(col(\"job_title\").rlike(\"data engineer\"), \"Data Engineer\")\n",
        "                   .when(col(\"job_title\").rlike(\"data architect\"), \"Data Architect\")\n",
        "                   .when(col(\"job_title\").rlike(\"data scientist\"), \"Data Science\")\n",
        "                   .when(col(\"job_title\").rlike(\"database\"), \"Data Management\")\n",
        "                   .when(col(\"job_title\").rlike(\"visualization\"), \"Data Analyst\")\n",
        "                   .otherwise(col(\"job_title_category\"))\n",
        "                  )"
      ],
      "outputs": [],
      "execution_count": 98,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "ms_comments": [
          {
            "threadId": "d984bd50-b7f9-44b1-a1dd-3384a699ff35",
            "text": "prepare the dataset by converting cell to its appropriate general form",
            "status": "active",
            "user": {
              "name": "Benedict Ibe",
              "idType": "aad"
            },
            "createdDateUTC": 1690493326072,
            "modifiedDateUTC": 1690493326072,
            "replies": []
          }
        ],
        "ms_comment_ranges": {
          "d984bd50-b7f9-44b1-a1dd-3384a699ff35": {
            "text": "     when",
            "start": {
              "line": 3,
              "column": 15
            },
            "end": {
              "line": 3,
              "column": 24
            }
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train2.show(200)"
      ],
      "outputs": [],
      "execution_count": 99,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE RANDOM FOREST TEXT CLASSIFICATION TO CATEGORIZE THE JOB_TITLE COLUMN"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer, StringIndexer, IndexToString\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col"
      ],
      "outputs": [],
      "execution_count": 100,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the column values from categorical to numerical using string indexer, so as to be suitable for running the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode the column values from categorical to numerical using string indexer, so as to be suitable for running the model\r\n",
        "Indexer = StringIndexer(inputCol=\"job_title_category\", outputCol=\"indexedLabel\").fit(df_train2)"
      ],
      "outputs": [],
      "execution_count": 101,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the training data with the encoded data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform the training data with the encoded data\r\n",
        "trainingDataIndexed = Indexer.transform(df_train2)"
      ],
      "outputs": [],
      "execution_count": 102,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store the encoded data (indexedLabel) into the labelCopy column created"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Store the encoded data (indexedLabel) into the labelCopy column created\r\n",
        "training_data_indexed = trainingDataIndexed.withColumn(\"labelCopy\", col(\"indexedLabel\"))"
      ],
      "outputs": [],
      "execution_count": 103,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize the training data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the training data\r\n",
        "tokenizer = Tokenizer(inputCol=\"job_title\", outputCol=\"words\")"
      ],
      "outputs": [],
      "execution_count": 104,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate other variables into a list called features"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate other variables into a list called features\r\n",
        "df_countVectorizer = CountVectorizer(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")"
      ],
      "outputs": [],
      "execution_count": 105,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the random forest algorithm\r\n",
        "df_rf = RandomForestClassifier(labelCol=\"labelCopy\", featuresCol=\"features\", numTrees=45, maxDepth=30)\r\n",
        "df_rf"
      ],
      "outputs": [],
      "execution_count": 106,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the encoded column back to the original string format, this works opposite with StringIndexer"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the encoded column back to the original string format, this works opposite with StringIndexer\n",
        "Converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
        "                               labels=Indexer.labels)"
      ],
      "outputs": [],
      "execution_count": 107,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Define a pipeline to pass the preparatory/pre-processing steps***"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[tokenizer, df_countVectorizer, df_rf, Converter])"
      ],
      "outputs": [],
      "execution_count": 108,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into test and train\r\n",
        "(train_set, test_set) = training_data_indexed.randomSplit([0.7, 0.3])"
      ],
      "outputs": [],
      "execution_count": 109,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test the model on the test dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model with the train set\r\n",
        "model = pipeline.fit(train_set)"
      ],
      "outputs": [],
      "execution_count": 110,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "make predictions of the built model on the main dataset"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Deploy the model on the test set\n",
        "predictions_test = model.transform(test_set)\n",
        "predictions_test.show()"
      ],
      "outputs": [],
      "execution_count": 111,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATING OUR MODEL"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelCopy\", predictionCol=\"prediction\",\n",
        "                                              metricName=\"accuracy\")"
      ],
      "outputs": [],
      "execution_count": 112,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the model accuracy"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluator.evaluate(predictions_test)\n",
        "print(\"Test Accuracy = %g\" % accuracy)"
      ],
      "outputs": [],
      "execution_count": 113,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2 = model.transform(df_clean2.withColumnRenamed(\"Candidate_job_title\", \"job_title\"))"
      ],
      "outputs": [],
      "execution_count": 114,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions2.show()"
      ],
      "outputs": [],
      "execution_count": 115,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select some columns to properly track the model\n",
        "predictions3 = predictions2.select('ID', 'WorkHistory_ID', 'job_title', 'predictedLabel', 'Duration')\n",
        "predictions3.show()"
      ],
      "outputs": [],
      "execution_count": 116,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper parameter tunning of random forest model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specify the grid values"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Specify the grid values\r\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\r\n",
        "\r\n",
        "paramGrid = ParamGridBuilder() \\\r\n",
        "    .addGrid(df_rf.numTrees, [15, 30, 45]) \\\r\n",
        "    .addGrid(df_rf.maxDepth, [10, 20, 30]) \\\r\n",
        "    .build()"
      ],
      "outputs": [],
      "execution_count": 117,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the cross validator\r\n",
        "cross_validator = CrossValidator(estimator=pipeline,\r\n",
        "                          estimatorParamMaps=paramGrid,\r\n",
        "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"labelCopy\", metricName=\"accuracy\"),\r\n",
        "                          numFolds=5, seed=42)"
      ],
      "outputs": [],
      "execution_count": 118,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Deploy the cross-validator on the training set\r\n",
        "new_model = cross_validator.fit(train_set)"
      ],
      "outputs": [],
      "execution_count": 119,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_param = new_model.bestModel"
      ],
      "outputs": [],
      "execution_count": 120,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the best parameters\r\n",
        "best_rf_model = best_param.stages[-2]\r\n",
        "\r\n",
        "bestParameter = {\r\n",
        "    \"numTrees\": best_rf_model.getNumTrees,\r\n",
        "    \"maxDepth\": best_rf_model.getMaxDepth(),\r\n",
        "}\r\n"
      ],
      "outputs": [],
      "execution_count": 121,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxDepthValue = best_rf_model.getMaxDepth()"
      ],
      "outputs": [],
      "execution_count": 122,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bestParameter)"
      ],
      "outputs": [],
      "execution_count": 123,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a logistic regression model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "outputs": [],
      "execution_count": 124,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize random forest model\r\n",
        "df_lr = LogisticRegression(labelCol=\"indexedLabel\", featuresCol=\"features\")"
      ],
      "outputs": [],
      "execution_count": 125,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model pipeline\r\n",
        "pipeline_lr = Pipeline(stages=[tokenizer, df_countVectorizer, df_lr, Converter])"
      ],
      "outputs": [],
      "execution_count": 126,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_lr = pipeline_lr.fit(train_set)"
      ],
      "outputs": [],
      "execution_count": 127,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_test_lr = model_lr.transform(test_set)"
      ],
      "outputs": [],
      "execution_count": 128,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model to get the accuracy"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\",\n",
        "                                              metricName=\"accuracy\")"
      ],
      "outputs": [],
      "execution_count": 129,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the accuracy of the model\n",
        "accuracy_lr = evaluator.evaluate(predictions_test_lr)\n",
        "print(\"Test Accuracy (Logistic Regression) = %g\" % accuracy_lr)"
      ],
      "outputs": [],
      "execution_count": 130,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_lr = model_lr.transform(df_clean2.withColumnRenamed(\"Candidate_job_title\", \"job_title\"))"
      ],
      "outputs": [],
      "execution_count": 131,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_lr.show()"
      ],
      "outputs": [],
      "execution_count": 132,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Track the predictions generated\n",
        "predictions_lr2 = predictions_lr.select('ID', 'Candidate_contact', 'WorkHistory_ID', 'job_title', 'predictedLabel', 'Duration')\n",
        "predictions_lr2.show()"
      ],
      "outputs": [],
      "execution_count": 133,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying the Decision tree model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "outputs": [],
      "execution_count": 134,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the decision tree algorithm\r\n",
        "df_dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", minInstancesPerNode = 1, maxDepth = 30, impurity = 'gini')"
      ],
      "outputs": [],
      "execution_count": 135,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the stage pipeline\r\n",
        "pipeline_dt = Pipeline(stages=[tokenizer, df_countVectorizer, df_dt, Converter])"
      ],
      "outputs": [],
      "execution_count": 136,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the decision tree model with the training data\r\n",
        "model_dt = pipeline_dt.fit(train_set)"
      ],
      "outputs": [],
      "execution_count": 137,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test the decision tree on the test data\r\n",
        "predictions_test_dt = model_dt.transform(test_set)"
      ],
      "outputs": [],
      "execution_count": 138,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model by generating the accuracy\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_dt = evaluator.evaluate(predictions_test_dt)"
      ],
      "outputs": [],
      "execution_count": 139,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Accuracy (Decision Tree) = %g\" % accuracy_dt)"
      ],
      "outputs": [],
      "execution_count": 140,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_dt = model_dt.transform(df_clean2.withColumnRenamed(\"Candidate_job_title\", \"job_title\"))\n",
        "predictions_dt.show()"
      ],
      "outputs": [],
      "execution_count": 141,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Track the predictions\n",
        "predictions_dt2 = predictions_dt.select('ID', 'Candidate_contact', 'WorkHistory_ID', 'job_title', 'predictedLabel', 'Duration')\n",
        "predictions_dt2.show()"
      ],
      "outputs": [],
      "execution_count": 142,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Hyper parameter tunning"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\r\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "outputs": [],
      "execution_count": 143,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the parameter search grid\r\n",
        "dt_paramGrid = ParamGridBuilder() \\\r\n",
        "    .addGrid(df_dt.maxDepth, [10, 20, 30]) \\\r\n",
        "    .addGrid(df_dt.minInstancesPerNode, [1, 2, 4]) \\\r\n",
        "    .addGrid(df_dt.impurity, [\"gini\", \"entropy\"]) \\\r\n",
        "    .build()"
      ],
      "outputs": [],
      "execution_count": 144,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the cross validator required to evaluate the model with the new parameters\r\n",
        "crossvalidator = CrossValidator(estimator=pipeline_dt,\r\n",
        "                          estimatorParamMaps=dt_paramGrid,\r\n",
        "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"labelCopy\", metricName=\"accuracy\"),\r\n",
        "                          numFolds=3)"
      ],
      "outputs": [],
      "execution_count": 145,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Search for the best parameters to train the dataset\r\n",
        "cvModel_DT = crossvalidator.fit(train_set) "
      ],
      "outputs": [],
      "execution_count": 146,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_param_dt = cvModel_DT.bestModel"
      ],
      "outputs": [],
      "execution_count": 147,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate the best parameters\r\n",
        "best_dt_model = best_param_dt.stages[-2]\r\n",
        "\r\n",
        "bestParameter_dt = {\r\n",
        "    \"minInstancesPerNode\": best_dt_model.getMinInstancesPerNode(),\r\n",
        "    \"maxDepth\": best_dt_model.getMaxDepth(),\r\n",
        "    \"impurity\": best_dt_model.getImpurity(),\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": 148,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bestParameter_dt)"
      ],
      "outputs": [],
      "execution_count": 149,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join the two tables at their candidate contact"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge the two tables at their common column (candidate contact)\n",
        "join_df = predictions_lr2.join(candidate_skilllevel, predictions_lr2.Candidate_contact == candidate_skilllevel.Candidate_contactno)\n",
        "join_df.show()"
      ],
      "outputs": [],
      "execution_count": 153,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore the merged dataframe\n",
        "new_df = join_df.select('Candidate_contact','Cand_ID', 'job_title', 'predictedLabel', 'Cand_skill_name', 'Cand_experience', 'Cand_level', 'Duration')\n",
        "new_df.show()"
      ],
      "outputs": [],
      "execution_count": 154,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the seniority level column\n",
        "new_df2 = new_df.withColumn(\"Seniority\",\n",
        "                   when(col(\"job_title\").rlike(\"(?i)senior\"), \"Senior\")\n",
        "                   .when(col(\"job_title\").rlike(\"(?i)junior\"), \"Junior\")\n",
        "                   .when(col(\"job_title\").rlike(\"(?i)mid\"), \"Mid\")\n",
        "                   .when(col(\"job_title\").rlike(\"(?i)head\"), \"Head\")\n",
        "                   .when(col(\"job_title\").rlike(\"(?i)lead\"), \"Lead\")\n",
        "                   .otherwise(\"None\"))\n"
      ],
      "outputs": [],
      "execution_count": 155,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df2.show(truncate = False)"
      ],
      "outputs": [],
      "execution_count": 156,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approach 2: Using Clustering"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.sql.functions import col"
      ],
      "outputs": [],
      "execution_count": 157,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize the tokenizer\r\n",
        "tokenizer = Tokenizer(inputCol=\"job_title\", outputCol=\"words\")"
      ],
      "outputs": [],
      "execution_count": 158,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the dataset\r\n",
        "df_tokenized = tokenizer.transform(new_df2)"
      ],
      "outputs": [],
      "execution_count": 159,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#embed the datatset with word2vec model\n",
        "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"words\", outputCol=\"features\")\n",
        "model = word2Vec.fit(df_tokenized)\n",
        "df_vectorized = model.transform(df_tokenized)"
      ],
      "outputs": [],
      "execution_count": 160,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans"
      ],
      "outputs": [],
      "execution_count": 161,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the value of k, and fit the k-means on the embedded dataset\n",
        "kmeans = KMeans().setK(10).setSeed(1)\n",
        "km_model = kmeans.fit(df_vectorized)\n",
        "df_clusters = km_model.transform(df_vectorized)"
      ],
      "outputs": [],
      "execution_count": 162,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clusters.select(\"job_title\", \"prediction\").show()"
      ],
      "outputs": [],
      "execution_count": 163,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
        "from pyspark.ml.clustering import BisectingKMeans\n",
        "from pyspark.sql.functions import col"
      ],
      "outputs": [],
      "execution_count": 164,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bisect = BisectingKMeans().setK(10).setSeed(1)  # Set K depending on how many broad categories you expect\r\n",
        "Bisect_model = Bisect.fit(df_vectorized)\r\n",
        "df_Bis_clusters = Bisect_model.transform(df_vectorized)"
      ],
      "outputs": [],
      "execution_count": 165,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_Bis_clusters.select(\"job_title\", \"prediction\").show(50)"
      ],
      "outputs": [],
      "execution_count": 166,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_workhist.show()"
      ],
      "outputs": [],
      "execution_count": 167,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_skilllevel.show()"
      ],
      "outputs": [],
      "execution_count": 168,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when, col, lit, regexp_extract"
      ],
      "outputs": [],
      "execution_count": 169,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate for each job_category"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate the merged dataset according to the job category\n",
        "grouped_new = new_df2.groupBy('predictedLabel','Candidate_contact', 'job_title', 'Cand_skill_name', 'Cand_experience', 'Cand_level', 'Duration', 'Seniority').count().sort('predictedLabel', ascending=False)\n",
        "grouped_new.show()"
      ],
      "outputs": [],
      "execution_count": 170,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate for each ID"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate the dataset according to each candidate\n",
        "grouped_ID = new_df2.groupBy('Candidate_contact','Cand_ID', 'job_title', 'predictedLabel', 'Cand_skill_name', 'Cand_experience', 'Cand_level','Duration', 'Seniority').count().sort('Cand_ID', ascending=False)\n",
        "grouped_ID.show(100,truncate = False)"
      ],
      "outputs": [],
      "execution_count": 171,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "\r\n",
        "aggregate_df = new_df2.groupBy('Candidate_contact','Cand_ID', 'predictedLabel', 'Cand_skill_name', 'Seniority').agg(F.mean(\"Duration\").alias(\"Average_Months\"))\r\n",
        "\r\n",
        "aggregate_df.show()"
      ],
      "outputs": [],
      "execution_count": 172,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the candidate ID\r\n",
        "candidateID = '9effb87c-d3f5-eb11-94ef-0022481a5643'"
      ],
      "outputs": [],
      "execution_count": 173,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resume Parser\r\n",
        "A resume skill extractor that extracts data related skills from a cv"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv = \"\"\"\r\n",
        "IFEANYI FRANKLIN NWOSU\r\n",
        "DATA SCIENTIST | ANALYST | SQL DEVELOPER\r\n",
        "07876582032 | Manchester, England | github.com/franklinn008 | franklinnwosu008@gmail.com\r\n",
        "linkedin.com/in/Ifeanyi-nwosu-523977169\r\n",
        "PROFESSIONAL PROFILE\r\n",
        "Experienced data professional recognised for driving data-driven decision-making and optimising\r\n",
        "organisational objectives. Proficient in Python, Jupyter, Excel, R, T-SQL, and data analysis tools (Apache\r\n",
        "Spark Databricks, PowerBI, Tableau, Microsoft Azure). Proven track record of delivering efficient results\r\n",
        "through innovative solutions. Committed to continuous self-development in the evolving field of data\r\n",
        "science\r\n",
        "EDUCATION\r\n",
        "MSc Data Science |University of Salford | September 2022 â September 2023\r\n",
        "â¢ Advanced Skill Development: Actively pursued self-development in the evolving field of data\r\n",
        "science. Developed comprehensive skills in R, PowerBI, and Tableau, enabling effective data\r\n",
        "analysis and visualization.\r\n",
        "â¢ Machine Learning Expertise: Gained practical experience in Python for Machine Learning and\r\n",
        "Data Mining. Successfully developed predictive models, extracting valuable insights from\r\n",
        "extensive datasets and driving data-driven decisions.\r\n",
        "â¢ Database Mastery: Enhanced proficiency in Advanced Databases through mastery of TSQL and\r\n",
        "SSMS. Efficiently managed databases conducted complex queries and ensured data integrity.\r\n",
        "â¢ Big Data Handling: Explored Big Data tools like Apache Data Bricks, enabling the handling and\r\n",
        "processing of massive datasets with speed and scalability.\r\n",
        "MSc Architecture |Nnamdi Azikiwe University| September 2015 - March 2018\r\n",
        "â¢ Architectural Proficiency: Demonstrated proficiency in architectural software, including AutoCAD,\r\n",
        "Revit, SketchUp, Lumion, and Artlantis, streamlining design and presentation processes.\r\n",
        "â¢ Critical Thinking & Soft Skills: Showcased critical thinking, effective communication, time\r\n",
        "management, adaptability, and attention to detail in job evaluations, emphasizing task\r\n",
        "prioritization.\r\n",
        "â¢ Integration of Expertise: Integrated technical expertise in architectural software, essential soft\r\n",
        "skills, and data analysis proficiency for successful project execution, consistently exceeding project\r\n",
        "expectations.\r\n",
        "BSc (Hons.) Architecture |Nnamdi Azikiwe University | September 2011 - July 2015\r\n",
        "SSCE |Special Science School | September 2003 - June 2009\r\n",
        "FSLC |Osumenyi central school | September 1997 - July 2003\r\n",
        "WORK EXPERIENCE/PROJECTS\r\n",
        "Data Specialist | Disruptive Learning Solutions (CINQ Game Developers) | France| June 2023 â To Date\r\n",
        "â¢ Data Cleaning & Transformation: Applied advanced data cleaning and transformation techniques\r\n",
        "to ensure data accuracy, consistency and readiness for analysis.\r\n",
        "â¢ Machine Learning Model Development: Developed predictive machine learning models using\r\n",
        "techniques such as predictive analytics and early action analysis to predict task time and failures\r\n",
        "accurately.\r\n",
        "â¢ Data-Driven Insights: Delivered actionable insights through data analysis, enabling informed\r\n",
        "decision-making in business and educational settings.\r\n",
        "â¢ User Engagement Optimization: Improved user engagement and satisfaction by analyzing user\r\n",
        "behaviour and feedback, achieving a significant 25% increase.\r\n",
        "â¢ Cross-functional collaboration: Collaborated with cross-functional teams to integrate data-driven\r\n",
        "features, ensuring holistic and effective solutions.\r\n",
        "â¢ Data Visualization: Created compelling data visualizations to convey insights, facilitating\r\n",
        "transparent and data-informed decision-making.\r\n",
        "Data Scientist/Analyst | DArTech Group | Manchester & Abuja| September 2022 â To Date\r\n",
        "â¢ Leadership & Innovation: Spearheaded the development of a Python-based classification model,\r\n",
        "harnessing ensemble methods and hyperparameter optimization to significantly enhance auction\r\n",
        "verification accuracy by 20%, reducing false positives and optimizing resources.\r\n",
        "â¢ Strategic Insights: Applied Python for customer segmentation clustering, identifying distinct\r\n",
        "customer groups based on purchasing patterns. Conducted sentiment analysis on extensive hotel\r\n",
        "reviews using NLP techniques, leading to informed marketing strategies and tailored customer\r\n",
        "experiences.\r\n",
        "â¢ Operational Efficiency: Engineered Python scripts automating complex tasks such as determining\r\n",
        "rollercoaster eligibility, lifespan calculations, BMI calculations, and tip bill calculations, resulting in\r\n",
        "a 30% reduction in time and effort.\r\n",
        "â¢ Data Visualization Mastery: Created captivating and interactive data visualizations using Tableau,\r\n",
        "PowerBI, Seaborn, and Matplotlib, integrating features like interactive filters, drill-down\r\n",
        "functionality, and custom visualizations. This enabled data-driven decision-making, contributing\r\n",
        "to a 15% increase in revenue.\r\n",
        "â¢ Statistical Proficiency: Conducted in-depth statistical analysis in R on world development\r\n",
        "indicators, employing techniques such as hypothesis testing and regression analysis to uncover\r\n",
        "critical patterns and relationships.\r\n",
        "â¢ Efficiency Enhancement: Analyzed large-scale clinical trial data using distributed computing\r\n",
        "frameworks, including RDD, DataFrame, and SQL within the Databricks platform, improving\r\n",
        "analytical efficiency by 25%.\r\n",
        "â¢ Database Excellence: Architected and developed a robust library management system and\r\n",
        "comprehensive pharmaceutical database using T-SQL. Ensured data integrity, retrieval efficiency,\r\n",
        "and security while streamlining operations.\r\n",
        "Facility/Building Data Manager | DIFF GLOBAL LINKS, [Medical Centre] | February 2021 â August 2022\r\n",
        "â¢ Strategic Data Management: Conducted meticulous data assessments to identify data sources,\r\n",
        "gaps, and integration opportunities within facility management processes, laying the groundwork\r\n",
        "for data-driven decision-making.\r\n",
        "â¢ Standardization & Efficiency: Introduced data management frameworks, standardizing data\r\n",
        "collection and storage, including SQL-based data management. This initiative empowered data\r\n",
        "analytics to transform complex data into actionable insights, leading to a 10% increase in energy\r\n",
        "efficiency\r\n",
        "Data Science Intern | Early Code Tech Institute Abuja | May 2022 â August 2022\r\n",
        "â¢ Hands-On Learning: Gained hands-on experience in data cleaning, preprocessing, and exploratory\r\n",
        "data analysis techniques, elevating proficiency in data wrangling and data quality assessment.\r\n",
        "â¢ Applied Knowledge: Developed skills in statistical analysis, machine learning algorithms, and\r\n",
        "predictive modelling, directly applying data science methodologies to real-world challenges and\r\n",
        "facilitating data-driven decisions.\r\n",
        "â¢ Effective Communication: Acquired proficiency in data visualization and storytelling, effectively\r\n",
        "communicating insights derived from data analysis to stakeholders and presenting findings with\r\n",
        "clarity and impact.\r\n",
        "REFERENCES:\r\n",
        "Available on request\r\n",
        "\"\"\""
      ],
      "outputs": [],
      "execution_count": 174,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv"
      ],
      "outputs": [],
      "execution_count": 175,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "outputs": [],
      "execution_count": 176,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a function to preprocess the cv text\r\n",
        "def resume_skills(text, skill_list):\r\n",
        "    skill = []\r\n",
        "    \r\n",
        "    for Candskill in skill_list:\r\n",
        "        pattern = r\"\\b{}\\b\".format(re.escape(Candskill))\r\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\r\n",
        "        if match:\r\n",
        "            skill.append(Candskill)\r\n",
        "    return skill"
      ],
      "outputs": [],
      "execution_count": 177,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Edensmith's skills list library**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crimson_skills = spark.read.load('abfss://enriched@esgadls2.dfs.core.windows.net/nurture/skills/skills_database/v1', format='delta'\r\n",
        "\r\n",
        ")\r\n",
        "\r\n",
        "crimson_skills.show(10)\r\n",
        "\r\n",
        "crimson_skills=crimson_skills.select(lower(col('crimson_name')))\r\n",
        "\r\n",
        "crimson_skills.show(10)\r\n",
        "\r\n",
        "skill_exists=crimson_skills.rdd.map(lambda x: x[0]).collect()"
      ],
      "outputs": [],
      "execution_count": 178,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skill_exists"
      ],
      "outputs": [],
      "execution_count": 179,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the skills from the cv"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\r\n",
        "    text = cv\r\n",
        "    \r\n",
        "    skill_list = skill_exists\r\n",
        "    \r\n",
        "    extracted_skills = resume_skills(text, skill_list)\r\n",
        "    \r\n",
        "    if extracted_skills:\r\n",
        "        print('Skills_extracted', extracted_skills)\r\n",
        "    else:\r\n",
        "            print('No skills matched')"
      ],
      "outputs": [],
      "execution_count": 180,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_skills"
      ],
      "outputs": [],
      "execution_count": 181,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the skills to lowercase\r\n",
        "extracted_skills2 = [skill.lower() for skill in extracted_skills]\r\n",
        "print(extracted_skills2)"
      ],
      "outputs": [],
      "execution_count": 182,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**View the career history of the Candidate selected from the database by his candidate ID**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ID = grouped_ID.filter(grouped_ID.Candidate_contact == candidateID)  \n",
        "df_ID.show(50, truncate=False)"
      ],
      "outputs": [],
      "execution_count": 183,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GROUPING AND AGGREGATING\r\n",
        "grouping to get the average duration of time for each candidate"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "aggregate_df = new_df2.groupBy('Candidate_contact', 'Cand_ID', 'predictedLabel', 'job_title', 'Cand_skill_name', 'Cand_experience', 'Cand_level', 'Seniority').agg(F.mean(\"Duration\").alias(\"TotalMonths\"))\n",
        "\n",
        "aggregate_df.show()\n"
      ],
      "outputs": [],
      "execution_count": 184,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CHECKING FOR THE RELATIONSHIP BETWEEN THE CANDIDATE EXPREIENCE, CANDIDATE SKILL LEVEL , AND NUMBER OF MONTHS/DURATION**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import corr"
      ],
      "outputs": [],
      "execution_count": 185,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import spearmanr"
      ],
      "outputs": [],
      "execution_count": 186,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ID2 = new_df2.toPandas()\n",
        "df_ID2"
      ],
      "outputs": [],
      "execution_count": 187,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ID3 = df_ID2[['Cand_level', 'Duration', 'Cand_experience']]\n",
        "df_ID3"
      ],
      "outputs": [],
      "execution_count": 188,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Treat for missing values\n",
        "df_clean = df_ID3.dropna()\n",
        "df_clean"
      ],
      "outputs": [],
      "execution_count": 189,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate the correlation matrix\n",
        "correlation_matrix = df_clean.corr()\n",
        "print(correlation_matrix)"
      ],
      "outputs": [],
      "execution_count": 190,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": 191,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the correlation Matrix\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": 192,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the parameters to be analysed"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_jobCategory = 'Data Science'"
      ],
      "outputs": [],
      "execution_count": 193,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_level = 'Senior'"
      ],
      "outputs": [],
      "execution_count": 194,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidateID = '9effb87c-d3f5-eb11-94ef-0022481a5643'"
      ],
      "outputs": [],
      "execution_count": 195,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candSkill = candidateID"
      ],
      "outputs": [],
      "execution_count": 196,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore the Candidate contact profile\r\n",
        "df_ID = grouped_ID.filter(grouped_ID.Candidate_contact == candidateID)  \r\n",
        "df_ID.show(50, truncate=False)"
      ],
      "outputs": [],
      "execution_count": 197,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ID_Skills = df_ID.select('Cand_skill_name').distinct()"
      ],
      "outputs": [],
      "execution_count": 198,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Current skills Of the candidate profile\r\n",
        "from pyspark.sql.functions import collect_list\r\n",
        "df_ID_list = df_ID_Skills.agg(collect_list(\"Cand_skill_name\")).first()[0]\r\n",
        "print(df_ID_list)"
      ],
      "outputs": [],
      "execution_count": 199,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Explore the target career\n",
        "df_target = grouped_ID.filter((grouped_ID.predictedLabel == target_jobCategory) & (grouped_ID.Seniority == target_level))\n",
        "df_target.show(50, truncate=False)"
      ],
      "outputs": [],
      "execution_count": 200,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_target2 = df_target.select('Cand_skill_name', 'Cand_experience', 'Cand_level', 'Duration')\n",
        "df_target2.show()"
      ],
      "outputs": [],
      "execution_count": 201,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GET THE TOP MOST COMMON SKILLS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregate to get the averave of Cand_skill_name, Cand_level, Cand_experience\r\n",
        "df_target3 = df_target2.groupBy('Cand_skill_name')\\\r\n",
        "    .agg(F.count('Cand_skill_name').alias('skill_count'),\r\n",
        "         F.mean(\"Cand_level\").alias(\"avg_skill_level\"),\r\n",
        "         F.mean(\"Cand_experience\").alias(\"avg_experience\")).orderBy('skill_count', ascending = False)\r\n",
        "\r\n",
        "df_target3.show()"
      ],
      "outputs": [],
      "execution_count": 202,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOST COMMON SKILLS OF THE TARGET CAREER"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the most common target skills\r\n",
        "most_important_skills = df_target3.filter(df_target3.skill_count > 50)\r\n",
        "most_important_skills.show()"
      ],
      "outputs": [],
      "execution_count": 203,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the average target skills\r\n",
        "mid_important_skills = df_target3.filter((df_target3.skill_count > 20) & (df_target3.skill_count < 100))\r\n",
        "mid_important_skills.show()"
      ],
      "outputs": [],
      "execution_count": 204,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the least target skills gap\r\n",
        "least_important_skills = df_target3.filter(df_target3.skill_count < 20)\r\n",
        "least_important_skills.show()"
      ],
      "outputs": [],
      "execution_count": 205,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the skill gap between the current and target jobs\n",
        "skill_gap = df_target2.filter(~col('Cand_skill_name').isin(candSkill)).distinct()\n",
        "skill_gap.show(truncate = False)"
      ],
      "outputs": [],
      "execution_count": 206,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#remove duplicated enteries\n",
        "skill_gap2 = skill_gap.dropDuplicates(['Cand_skill_name'])\n",
        "skill_gap2.show(truncate = False)"
      ],
      "outputs": [],
      "execution_count": 207,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RECOMMENDATION ANALYSIS"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#most recommended skills to learn\r\n",
        "top_skills = most_important_skills.filter(~col('Cand_skill_name')\\\r\n",
        ".isin(candSkill)).distinct().orderBy('skill_count', ascending = False)\r\n",
        "top_skills.show(10,truncate = False)"
      ],
      "outputs": [],
      "execution_count": 208,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mid Important Skills"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mid recommended skills to learn\r\n",
        "mid_skills = mid_important_skills.filter(~col('Cand_skill_name')\\\r\n",
        ".isin(candSkill)).distinct().orderBy('skill_count', ascending = False)\r\n",
        "mid_skills.show(10,truncate = False)"
      ],
      "outputs": [],
      "execution_count": 209,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Least Important skill"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#most recommended skills to learn\r\n",
        "least_skills = least_important_skills.filter(~col('Cand_skill_name')\\\r\n",
        ".isin(candSkill)).distinct().orderBy('skill_count', ascending = False)\r\n",
        "least_skills.show(10,truncate = False)"
      ],
      "outputs": [],
      "execution_count": 210,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_skillsPD = top_skills.toPandas()\r\n",
        "top_skillsPD"
      ],
      "outputs": [],
      "execution_count": 211,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mid_skillsPD = mid_skills.toPandas()"
      ],
      "outputs": [],
      "execution_count": 212,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "least_skillsPD = least_skills.toPandas()"
      ],
      "outputs": [],
      "execution_count": 213,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the skill gap\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "sns.set_style('whitegrid')\r\n",
        "sns.barplot(x='skill_count', y='Cand_skill_name', data=top_skillsPD.head(10))\r\n",
        "plt.xlabel('Frequency')\r\n",
        "plt.ylabel('Cand_skill_name')\r\n",
        "sns.despine(left=True, bottom=True)"
      ],
      "outputs": [],
      "execution_count": 214,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crimson_skills = spark.read.load('abfss://enriched@esgadls2.dfs.core.windows.net/nurture/skills/skills_database/v1', format='delta'\r\n",
        "\r\n",
        ")\r\n",
        "\r\n",
        "crimson_skills.show(10)\r\n",
        "\r\n",
        "crimson_skills=crimson_skills.select(lower(col('crimson_name')))\r\n",
        "\r\n",
        "crimson_skills.show(10)\r\n",
        "\r\n",
        "skill_exists=crimson_skills.rdd.map(lambda x: x[0]).collect()"
      ],
      "outputs": [],
      "execution_count": 215,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "account_name = 'esgadls2' # fill in your primary account name\r\n",
        "\r\n",
        "container_name = 'enriched' # fill in your container name\r\n",
        "\r\n",
        "table_name  = 'career_recommender_joinedDF'\r\n",
        "\r\n",
        "version_num = 'v1'\r\n",
        "\r\n",
        "relative_path = '/recruitment/%s/%s' %(table_name,version_num) # fill in your relative folder path\r\n",
        "\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "# Create table in the metastore using DataFrame's schema and write data to it\r\n",
        "\r\n",
        "join_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"esgmain.%s\" %(table_name))\r\n",
        "\r\n",
        "# Create or replace partitioned table with path using DataFrame's schema and write/overwrite data to it\r\n",
        "\r\n",
        "join_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(adls_path)"
      ],
      "outputs": [],
      "execution_count": 216,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_retrieve = spark.read.load('abfss://enriched@esgadls2.dfs.core.windows.net/recruitment/career_recommender_joinedDF/v1', format='delta'\r\n",
        "\r\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 217,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "career_recommender_joinedDF.show()"
      ],
      "outputs": [],
      "execution_count": 218,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}